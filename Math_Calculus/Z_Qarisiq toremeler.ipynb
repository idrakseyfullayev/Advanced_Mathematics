{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84655251",
   "metadata": {},
   "source": [
    "## QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™ri tapmaq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e656aadd",
   "metadata": {},
   "source": [
    "QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™ri tapmaq Ã¼Ã§Ã¼n É™vvÉ™lcÉ™ birinci qismÉ™n tÃ¶rÉ™mÉ™lÉ™ri, sonra isÉ™ ikinci dÉ™rÉ™cÉ™li qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™ri hesablayÄ±rÄ±q. VerilmiÅŸ funksiyanÄ± yenidÉ™n qeyd edÉ™k:  \n",
    "\n",
    "$$\n",
    "f(x, y) = x^3 y^2 + 5xy^3\n",
    "$$\n",
    "\n",
    "### 1. **$ x $-É™ gÃ¶rÉ™ qismÉ™n tÃ¶rÉ™mÉ™** (artÄ±q tapÄ±lmÄ±ÅŸdÄ±):\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = 3x^2 y^2 + 5y^3\n",
    "$$\n",
    "\n",
    "Bunu **$ y $-yÉ™ gÃ¶rÉ™** tÃ¶rÉ™tsÉ™k:\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial}{\\partial y} (3x^2 y^2 + 5y^3)\n",
    "$$\n",
    "- $ 3x^2 y^2 $ ifadÉ™sinin $ y $-yÉ™ gÃ¶rÉ™ tÃ¶rÉ™mÉ™si: $ 6x^2 y $\n",
    "- $ 5y^3 $ ifadÉ™sinin $ y $-yÉ™ gÃ¶rÉ™ tÃ¶rÉ™mÉ™si: $ 15y^2 $\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} = 6x^2 y + 15y^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **$ y $-yÉ™ gÃ¶rÉ™ qismÉ™n tÃ¶rÉ™mÉ™** (artÄ±q tapÄ±lmÄ±ÅŸdÄ±):\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial y} = 2x^3 y + 15xy^2\n",
    "$$\n",
    "\n",
    "Bunu **$ x $-É™ gÃ¶rÉ™** tÃ¶rÉ™tsÉ™k:\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial x} (2x^3 y + 15xy^2)\n",
    "$$\n",
    "- $ 2x^3 y $ ifadÉ™sinin $ x $-É™ gÃ¶rÉ™ tÃ¶rÉ™mÉ™si: $ 6x^2 y $\n",
    "- $ 15xy^2 $ ifadÉ™sinin $ x $-É™ gÃ¶rÉ™ tÃ¶rÉ™mÉ™si: $ 15y^2 $\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial y} = 6x^2 y + 15y^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **NÉ™ticÉ™**  \n",
    "Simmetriya qaydasÄ±na gÃ¶rÉ™ qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r eynidir:\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial^2 f}{\\partial x \\partial y} = 6x^2 y + 15y^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565bd745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "529f8e26",
   "metadata": {},
   "source": [
    "##  QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r eynidirsÉ™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6e4d3a",
   "metadata": {},
   "source": [
    "QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r eynidirsÉ™, **analitik hÉ™ll** var yÉ™ni  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial^2 f}{\\partial x \\partial y}\n",
    "$$\n",
    "\n",
    "bu o demÉ™kdir ki, funksiyanÄ±n ikinci dÉ™rÉ™cÉ™li qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™ri **Clairautun teoreminÉ™** gÃ¶rÉ™ bÉ™rabÉ™rdir vÉ™ $ f(x, y) $ **funksiyasÄ± kifayÉ™t qÉ™dÉ™r hamar (davamlÄ± ikinci dÉ™rÉ™cÉ™li tÃ¶rÉ™mÉ™lÉ™rÉ™ malik) bir funksiyadÄ±r**.  \n",
    "\n",
    "Bu isÉ™ o demÉ™kdir ki, funksiyanÄ± analitik Ã¼sullarla tÉ™dqiq etmÉ™k mÃ¼mkÃ¼ndÃ¼r vÉ™ diferensial tÉ™nliklÉ™rdÉ™ tÉ™tbiq oluna bilÉ™r. YÉ™ni, **bu funksiyanÄ± tÉ™hlil vÉ™ inteqrasiya etmÉ™kdÉ™ heÃ§ bir problem yoxdur**.\n",
    "\n",
    "---\n",
    "\n",
    "**QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r** eynidirsÉ™ **Gradient Descent** lazÄ±m deyil!  \n",
    "\n",
    "Ã‡Ã¼nki funksiyanÄ±n **tÃ¶rÉ™mÉ™lÉ™ri analitik ÅŸÉ™kildÉ™ tapÄ±lÄ±r vÉ™ hesablamaq asandÄ±r**. Gradient Descent É™sasÉ™n:  \n",
    "- TÃ¶rÉ™mÉ™lÉ™ri analitik hesablamaq Ã§É™tin vÉ™ ya mÃ¼mkÃ¼n olmadÄ±qda,  \n",
    "- Ã‡ox Ã¶lÃ§Ã¼lÃ¼ vÉ™ mÃ¼rÉ™kkÉ™b funksiyalar Ã¼Ã§Ã¼n optimal hÉ™ll tapmaq lazÄ±m gÉ™ldikdÉ™ istifadÉ™ olunur.  \n",
    "\n",
    "Bu halda, sadÉ™ **analitik tÃ¶rÉ™mÉ™** ilÉ™ lokal vÉ™ qlobal minimum/maksimumu tapa bilÉ™rik. ÆgÉ™r kritik nÃ¶qtÉ™lÉ™ri tapmaq lazÄ±mdÄ±rsa,  \n",
    "\n",
    "$$\n",
    "\\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right) = (0,0)\n",
    "$$\n",
    "\n",
    "bÉ™rabÉ™rliyini hÉ™ll edÉ™rÉ™k stasionar nÃ¶qtÉ™lÉ™ri tapa bilÉ™rik. ÆgÉ™r mÉ™qsÉ™diniz budursa, hesablaya bilÉ™rÉ™m. ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac303c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b75b0899",
   "metadata": {},
   "source": [
    "## QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r bÉ™rabÉ™rdirsÉ™ Gradient Descent lazÄ±m deyil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27773210",
   "metadata": {},
   "source": [
    "**Gradient Descent**-É™ ehtiyac olub-olmamasÄ± É™sasÉ™n **funksiyanÄ±n tÃ¶rÉ™mÉ™lÉ™rinin hesablana bilmÉ™ vÉ™ziyyÉ™tindÉ™n vÉ™ optimallaÅŸdÄ±rma ehtiyacÄ±ndan** asÄ±lÄ±dÄ±r.  \n",
    "\n",
    "### 1ï¸âƒ£ **QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r bÉ™rabÉ™rdirsÉ™ Gradient Descent lazÄ±m deyil**  \n",
    "ÆgÉ™r qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r **analitik ÅŸÉ™kildÉ™** hesablanÄ±bsa vÉ™ aÃ§Ä±q formada ifadÉ™ edilÉ™ bilirsÉ™, **Gradient Descent lazÄ±m deyil**. Ã‡Ã¼nki optimal nÃ¶qtÉ™lÉ™ri **stasionar nÃ¶qtÉ™lÉ™ri tapÄ±b** analiz edÉ™rÉ™k mÃ¼É™yyÉ™n edÉ™ bilÉ™rik.  \n",
    "\n",
    "**Misal:**  \n",
    "ÆgÉ™r $ f(x, y) = x^3 y^2 + 5xy^3 $ Ã¼Ã§Ã¼n qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial^2 f}{\\partial x \\partial y} = 6x^2 y + 15y^2\n",
    "$$\n",
    "\n",
    "kimi **sadÉ™ vÉ™ hesablana bilÉ™n** ifadÉ™lÉ™rdirsÉ™, **Gradient Descent lazÄ±m deyil**, Ã§Ã¼nki biz onlarÄ± **tÉ™nlik kimi hÉ™ll edÉ™ bilÉ™rik**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ **Gradient Descent nÉ™ vaxt lazÄ±mdÄ±r?**  \n",
    "ÆgÉ™r qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r:  \n",
    "âœ… **Ã‡ox mÃ¼rÉ™kkÉ™bdirsÉ™** vÉ™ **É™l ilÉ™ hÉ™lli Ã§É™tindirsÉ™**,  \n",
    "âœ… **Qeyri-xÉ™tti diferensial tÉ™nliklÉ™r verirsÉ™** vÉ™ **algebraik olaraq hÉ™ll edilÉ™ bilmirsÉ™**,  \n",
    "âœ… **HÉ™lli Ã¼Ã§Ã¼n iterativ metodlar tÉ™lÉ™b olunursa**,  \n",
    "\n",
    "bu zaman **Gradient Descent vÉ™ ya digÉ™r optimallaÅŸdÄ±rma Ã¼sullarÄ±** lazÄ±m ola bilÉ™r.  \n",
    "\n",
    "**Misal:**  \n",
    "ÆgÉ™r $ f(x, y) $ elÉ™ bir funksiya olsaydÄ± ki, onun tÃ¶rÉ™mÉ™lÉ™ri Ã§ox mÃ¼rÉ™kkÉ™b bir sistem É™mÉ™lÉ™ gÉ™tirirdi:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = e^{x^2 + y^2} (2x + 3y^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial y} = \\ln(x^2 + y^2) (x - 4y^3)\n",
    "$$\n",
    "\n",
    "bu halda **É™l ilÉ™ hÉ™ll Ã§ox Ã§É™tin olardÄ±** vÉ™ **Gradient Descent vÉ™ ya Newtonâ€™s Method** kimi optimallaÅŸdÄ±rma Ã¼sullarÄ± tÉ™tbiq edilmÉ™li olardÄ±.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ **Yekun NÉ™ticÉ™**  \n",
    "ğŸ’¡ **ÆgÉ™r qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r aÃ§Ä±q ÅŸÉ™kildÉ™ ifadÉ™ edilirsÉ™ vÉ™ onlarÄ±n sÄ±fÄ±ra bÉ™rabÉ™r olduÄŸu nÃ¶qtÉ™lÉ™ri tapmaq mÃ¼mkÃ¼ndÃ¼rsÉ™, Gradient Descent lazÄ±m deyil.**  \n",
    "\n",
    "ğŸ” **ÆgÉ™r qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r Ã§ox mÃ¼rÉ™kkÉ™b vÉ™ ya qeyri-xÉ™tti bir sistem É™mÉ™lÉ™ gÉ™tirirsÉ™, Gradient Descent kimi iterativ metodlar istifadÉ™ edilÉ™ bilÉ™r.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a781d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21a001d1",
   "metadata": {},
   "source": [
    "## QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a40ec5c",
   "metadata": {},
   "source": [
    "**QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ± analitik hÉ™llin olmamasÄ± demÉ™k deyil.**  \n",
    "\n",
    "ÆslindÉ™, qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ± funksiyanÄ±n mÃ¼É™yyÉ™n istiqamÉ™tlÉ™rdÉ™ **dÉ™yiÅŸmÉ™mÉ™si** vÉ™ ya **dÃ¼z xÉ™tt boyunca sabit qalmasÄ±** kimi hallarÄ± ifadÉ™ edÉ™ bilÉ™r. Bu, hÉ™miÅŸÉ™ analitik hÉ™llin olmamasÄ± anlamÄ±na gÉ™lmir. Daha dÉ™qiq izah edÉ™k:  \n",
    "\n",
    "---  \n",
    "\n",
    "## **1ï¸âƒ£ QarÄ±ÅŸÄ±q TÃ¶rÉ™mÉ™ vÉ™ Onun SÄ±fÄ±r OlmasÄ±**  \n",
    "QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™ dedikdÉ™, bir funksiyanÄ±n iki dÉ™yiÅŸÉ™nÉ™ gÃ¶rÉ™ ikinci dÉ™rÉ™cÉ™li tÃ¶rÉ™mÉ™si nÉ™zÉ™rdÉ™ tutulur:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial y}\n",
    "$$  \n",
    "\n",
    "ÆgÉ™r bu dÉ™yÉ™r **bÃ¼tÃ¼n nÃ¶qtÉ™lÉ™rdÉ™ sÄ±fÄ±rdÄ±rsa**, bu o demÉ™kdir ki, $ f(x, y) $-nin dÉ™yiÅŸmÉ™ sÃ¼rÉ™ti **x vÉ™ y dÉ™yiÅŸÉ™nlÉ™ri Ã¼zrÉ™ qarÅŸÄ±lÄ±qlÄ± tÉ™sir gÃ¶stÉ™rmir**.  \n",
    "\n",
    "Bu, iki mÃ¼mkÃ¼n vÉ™ziyyÉ™ti gÃ¶stÉ™rir:  \n",
    "- Funksiya **ayrÄ±lÄ±qda iki dÉ™yiÅŸÉ™nÉ™ gÃ¶rÉ™ xÉ™tti ola bilÉ™r**, mÉ™sÉ™lÉ™n:    \n",
    "  $$\n",
    "  f(x, y) = ax + by + c\n",
    "  $$  \n",
    "  Bu halda funksiya Ã§ox sadÉ™dir vÉ™ **analitik hÉ™ll tam mÃ¼mkÃ¼ndÃ¼r**.  \n",
    "- ÆgÉ™r funksiya daha mÃ¼rÉ™kkÉ™b bir quruluÅŸa malikdirsÉ™ vÉ™ **stasionar nÃ¶qtÉ™lÉ™rÉ™ malikdirsÉ™**, iterativ metodlar (mÉ™sÉ™lÉ™n, **Gradient Descent**) tÉ™lÉ™b oluna bilÉ™r.  \n",
    "\n",
    "---  \n",
    "\n",
    "## **2ï¸âƒ£ QarÄ±ÅŸÄ±q TÃ¶rÉ™mÉ™lÉ™rin SÄ±fÄ±r OlmasÄ± Analitik HÉ™llin OlmamasÄ± DemÉ™kdirmi?**  \n",
    "- **Xeyr, hÉ™r zaman yox!**    \n",
    "  QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ± funksiyanÄ±n **ayrÄ±lÄ±qda $ x $ vÉ™ $ y $-yÉ™ gÃ¶rÉ™ asanlÄ±qla hÉ™ll edilÉ™ bilÉ™cÉ™yini** gÃ¶stÉ™rÉ™ bilÉ™r. BelÉ™ hallarda analitik hÉ™ll mÃ¶vcuddur vÉ™ iterativ metodlara ehtiyac olmur.  \n",
    "\n",
    "- **BÉ™zÉ™n iterativ metodlar tÉ™lÉ™b oluna bilÉ™r.**    \n",
    "  ÆgÉ™r funksiya **tÃ¶rÉ™mÉ™lÉ™ri sÄ±fÄ±r edÉ™n bir nÃ¶qtÉ™yÉ™ malikdirsÉ™ vÉ™ hÉ™min nÃ¶qtÉ™dÉ™ ikinci dÉ™rÉ™cÉ™li tÃ¶rÉ™mÉ™lÉ™r funksiyanÄ±n tipliliyini (minimum, maksimum, saddle point) tam mÃ¼É™yyÉ™n edÉ™ bilmirsÉ™**, Gradient Descent kimi **iterativ metodlar** istifadÉ™ edilÉ™ bilÉ™r.  \n",
    "\n",
    "---  \n",
    "\n",
    "## **3ï¸âƒ£ NÃ¼munÉ™lÉ™r ilÉ™ Ä°zah**  \n",
    "### **A. Analitik hÉ™llin mÃ¼mkÃ¼n olduÄŸu hallar**  \n",
    "ÆgÉ™r $ f(x, y) $ aÅŸaÄŸÄ±dakÄ± kimi sadÉ™dirsÉ™:  \n",
    "\n",
    "$$\n",
    "f(x, y) = 3x + 5y\n",
    "$$  \n",
    "\n",
    "Bu funksiyanÄ±n tÃ¶rÉ™mÉ™lÉ™ri belÉ™ olacaq:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = 3, \\quad \\frac{\\partial f}{\\partial y} = 5, \\quad \\frac{\\partial^2 f}{\\partial x \\partial y} = 0\n",
    "$$  \n",
    "\n",
    "Burada qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™ sÄ±fÄ±rdÄ±r, amma funksiya **sadÉ™ xÉ™ttidir vÉ™ analitik hÉ™ll aÃ§Ä±q ÅŸÉ™kildÉ™ mÃ¼mkÃ¼ndÃ¼r.** Burada heÃ§ bir optimallaÅŸdÄ±rma vÉ™ Gradient Descent tÉ™lÉ™b olunmur.  \n",
    "\n",
    "---  \n",
    "\n",
    "### **B. Gradient Descent TÉ™lÉ™b EdilÉ™n Hal**  \n",
    "ÆgÉ™r funksiyanÄ±z belÉ™dirsÉ™:  \n",
    "\n",
    "$$\n",
    "f(x, y) = x^2 y^2 + 5xy\n",
    "$$  \n",
    "\n",
    "Bunun qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™sini tapaq:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y} (2xy^2 + 5y) = 4xy + 5\n",
    "$$  \n",
    "\n",
    "Bu, mÃ¼É™yyÉ™n nÃ¶qtÉ™lÉ™rdÉ™ sÄ±fÄ±r ola bilÉ™r:  \n",
    "\n",
    "$$\n",
    "4xy + 5 = 0 \\quad \\Rightarrow \\quad xy = -\\frac{5}{4}\n",
    "$$  \n",
    "\n",
    "Bu halda, **stasionar nÃ¶qtÉ™lÉ™ri tapmaq Ã¼Ã§Ã¼n Gradient Descent kimi metodlara ehtiyac ola bilÉ™r**, Ã§Ã¼nki funksiyanÄ±n **qeyri-xÉ™tti olmasÄ± vÉ™ Ã§oxlu lokal ekstremumlara malik olmasÄ± mÃ¼mkÃ¼ndÃ¼r.**  \n",
    "\n",
    "---  \n",
    "\n",
    "## **4ï¸âƒ£ Son NÉ™ticÉ™**  \n",
    "- **QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ± hÉ™miÅŸÉ™ analitik hÉ™llin olmamasÄ± demÉ™k deyil.**    \n",
    "- **ÆgÉ™r funksiya sadÉ™dirsÉ™, analitik hÉ™ll mÃ¼mkÃ¼ndÃ¼r.**    \n",
    "- **ÆgÉ™r funksiya mÃ¼rÉ™kkÉ™b vÉ™ qeyri-xÉ™ttidirsÉ™, stasionar nÃ¶qtÉ™lÉ™rin tapÄ±lmasÄ± Ã¼Ã§Ã¼n Gradient Descent kimi optimallaÅŸdÄ±rma metodlarÄ± lazÄ±m ola bilÉ™r.**    \n",
    "\n",
    "ÆgÉ™r konkret bir funksiya Ã¼zÉ™rindÉ™ araÅŸdÄ±rma aparmaq istÉ™yirsinizsÉ™, nÃ¼munÉ™ verin, birlikdÉ™ baxaq! ğŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be825ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "077eb842",
   "metadata": {},
   "source": [
    "## QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r sÄ±fÄ±rdÄ±rsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54370863",
   "metadata": {},
   "source": [
    " ÆgÉ™r **qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r sÄ±fÄ±rdÄ±rsa** (yÉ™ni funksiyanÄ±n gradienti sÄ±fÄ±ra bÉ™rabÉ™rdirsÉ™), o zaman **Gradient Descent** istifadÉ™ etmÉ™yin É™sas mÉ™qsÉ™di **optimal hÉ™llÉ™ Ã§atmaqdÄ±r**.  \n",
    "\n",
    "### 1ï¸âƒ£ **QarÄ±ÅŸÄ±q TÃ¶rÉ™mÉ™lÉ™r SÄ±fÄ±ra BÉ™rabÉ™rdirsÉ™:**  \n",
    "\n",
    "ÆgÉ™r $ \\nabla f(x, y) = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right) = (0, 0) $, yÉ™ni funksiyanÄ±n gradienti sÄ±fÄ±rdÄ±rsa, bu, **stasionar nÃ¶qtÉ™** olduÄŸunu bildirir. Ancaq bu nÃ¶qtÉ™ **minimum**, **maksimum** vÉ™ ya **saddlÉ™ÅŸmiÅŸ nÃ¶qtÉ™** (saddle point) ola bilÉ™r.  \n",
    "\n",
    "Bu halda, **Gradient Descent** istifadÉ™ etmÉ™k vacibdir, Ã§Ã¼nki bu metod **stasionar nÃ¶qtÉ™lÉ™ri tapmaq Ã¼Ã§Ã¼n iterativ olaraq minimuma yaxÄ±nlaÅŸmaq** Ã¼Ã§Ã¼n istifadÉ™ olunur. Gradient Descent-in É™sas mÉ™qsÉ™di **minimum nÃ¶qtÉ™sinÉ™ doÄŸru hÉ™rÉ™kÉ™t etmÉ™k**dir, xÃ¼susÉ™n dÉ™ analitik hÉ™ll tapmaq Ã§É™tin vÉ™ ya mÃ¼mkÃ¼n olmadÄ±qda.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ **Gradient Descent necÉ™ iÅŸlÉ™yir?**  \n",
    "Gradient Descent metodunun É™sas ideyasÄ± funksiyanÄ±n **gradientinÉ™ É™saslanaraq** (yÉ™ni tÃ¶rÉ™mÉ™lÉ™rinÉ™ É™saslanaraq) funksiyanÄ± minimuma doÄŸru optimallaÅŸdÄ±rmaqdÄ±r. HÉ™r iterasiyada, cari nÃ¶qtÉ™dÉ™n funksiyanÄ±n gradientinÉ™ É™saslanaraq nÃ¶vbÉ™ti nÃ¶qtÉ™ hesablanÄ±r.  \n",
    "\n",
    "**Gradient Descent formulasÄ±:**  \n",
    "\n",
    "$$\n",
    "\\theta_{new} = \\theta_{old} - \\alpha \\nabla f(\\theta)\n",
    "$$  \n",
    "\n",
    "- $ \\theta $ - parametrlÉ™r (burada $ x $ vÉ™ $ y $ ola bilÉ™r)  \n",
    "- $ \\alpha $ - Ã¶yrÉ™nmÉ™ sÃ¼rÉ™ti (learning rate)  \n",
    "- $ \\nabla f(\\theta) $ - funksiyanÄ±n gradienti (tÃ¶rÉ™mÉ™lÉ™ri)  \n",
    "\n",
    "Bu tÉ™nlikdÉ™ $ \\alpha $ Ã§ox bÃ¶yÃ¼k vÉ™ ya Ã§ox kiÃ§ik seÃ§ildikdÉ™, optimallaÅŸdÄ±rma prosesi ya Ã§ox sÃ¼rÉ™tli, ya da Ã§ox yavaÅŸ ola bilÉ™r. ÆgÉ™r $ \\alpha $ Ã§ox bÃ¶yÃ¼kdÃ¼rsÉ™, adÉ™tÉ™n hÉ™llin optimallaÅŸmasÄ± tÉ™krarlanmaz, Ã§Ã¼nki hÉ™r addÄ±mda Ã§ox bÃ¶yÃ¼k dÉ™yiÅŸikliklÉ™r baÅŸ verÉ™r.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ **Gradient Descent NÉ™ Vaxt TÉ™tbiq Edilir?**  \n",
    "Gradient Descent aÅŸaÄŸÄ±dakÄ± hallarda istifadÉ™ olunur:  \n",
    "\n",
    "- **FunksiyanÄ±n analitik olaraq hÉ™lli mÃ¼mkÃ¼n deyil** vÉ™ ya Ã§ox mÃ¼rÉ™kkÉ™bdir (funksiyanÄ±n tÃ¶rÉ™mÉ™si vÉ™ ya sÄ±fÄ±ra bÉ™rabÉ™r olduÄŸu nÃ¶qtÉ™lÉ™r É™l ilÉ™ tapÄ±la bilmÉ™z).  \n",
    "- **Funksiya Ã§ox bÃ¶yÃ¼k Ã¶lÃ§Ã¼lÃ¼ vÉ™ ya qeyri-xÉ™tti** olur (mÉ™sÉ™lÉ™n, Ã§ox sayda dÉ™yiÅŸÉ™ni olan vÉ™ ya Ã§ox mÃ¼rÉ™kkÉ™b formullarÄ± olan).  \n",
    "- **Ã‡oxsaylÄ± stasionar nÃ¶qtÉ™lÉ™r mÃ¶vcuddur** vÉ™ optimal hÉ™llin tapÄ±lmasÄ± Ã§É™tindir (minimallaÅŸdÄ±rma vÉ™ ya optimallaÅŸdÄ±rma tapÅŸÄ±rÄ±qlarÄ±).  \n",
    "\n",
    "---\n",
    "\n",
    "### 4ï¸âƒ£ **NÉ™ticÉ™**  \n",
    "ÆgÉ™r qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r sÄ±fÄ±rdÄ±rsa vÉ™ funksiyanÄ±n analitik hÉ™lli Ã§É™tindirsÉ™, **Gradient Descent** istifadÉ™ etmÉ™k Ã§ox faydalÄ±dÄ±r. Bu metod funksiyanÄ± **iterativ ÅŸÉ™kildÉ™ optimallaÅŸdÄ±rmaÄŸa** kÃ¶mÉ™k edir vÉ™ yerli minimumu tapmaq Ã¼Ã§Ã¼n istifadÉ™ edilir.  \n",
    "\n",
    "EhtiyacÄ±nÄ±z varsa, **Gradient Descent**-in Python-da tÉ™tbiqi Ã¼Ã§Ã¼n nÃ¼munÉ™ verÉ™ bilÉ™rÉ™m! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee768c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6c93ac3",
   "metadata": {},
   "source": [
    "## ÃœmumilÉ™ÅŸdrimÉ™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d484e",
   "metadata": {},
   "source": [
    "### 1ï¸âƒ£ **QarÄ±ÅŸÄ±q TÃ¶rÉ™mÉ™lÉ™rin BÉ™rabÉ™rliyi:**  \n",
    "ÆgÉ™r **qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r bÉ™rabÉ™rdirsÉ™**, yÉ™ni  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial^2 f}{\\partial x \\partial y},\n",
    "$$  \n",
    "\n",
    "bu, **Clairaut-un teoremi**nÉ™ gÃ¶rÉ™ **funksiyanÄ±n hÉ™m $ x $-É™, hÉ™m dÉ™ $ y $-yÉ™ gÃ¶rÉ™ tÃ¼revlerinin mÃ¼badilÉ™si mÃ¼mkÃ¼n olduÄŸunu** bildirir. Bu halda **funksiya hamar (differensiyalanabilÉ™n)** sayÄ±lÄ±r vÉ™ **analitik hÉ™ll** mÃ¼mkÃ¼ndÃ¼r. YÉ™ni, bu halda funksiyanÄ±n **tÃ¶rÉ™mÉ™lÉ™ri mÃ¶vcuddur vÉ™ dÃ¼zgÃ¼n bir ÅŸÉ™kildÉ™ hesablanÄ±r**.  \n",
    "\n",
    "**QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin bÉ™rabÉ™r olmasÄ±** funksiyanÄ±n analitik hÉ™lli olduÄŸu anlamÄ±na gÉ™lir, Ã§Ã¼nki bu, funksiyanÄ±n **davamlÄ± vÉ™ diferensiyalanabilÉ™n olduÄŸunu** gÃ¶stÉ™rir.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ **QarÄ±ÅŸÄ±q TÃ¶rÉ™mÉ™lÉ™rin SÄ±fÄ±r OlmasÄ±:**  \n",
    "ÆgÉ™r **qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™r sÄ±fÄ±rdÄ±rsa** (yÉ™ni, bir vÉ™ ya daha Ã§ox tÃ¶rÉ™mÉ™ sÄ±fÄ±ra bÉ™rabÉ™rdirsÉ™), bu, funksiyanÄ±n **stasionar nÃ¶qtÉ™lÉ™rÉ™ sahib olduÄŸunu** bildirÉ™ bilÉ™r. Lakin **qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ±** funksiyanÄ±n **hamar olmamasÄ±** demÉ™k deyil. Funksiya **davamlÄ±** vÉ™ **diferensiyalanabilÉ™n** ola bilÉ™r, amma bu, sadÉ™cÉ™ onun **minimum vÉ™ ya maksimum nÃ¶qtÉ™ olduÄŸunu** gÃ¶stÉ™rir.  \n",
    "\n",
    "**QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ±**, optimallaÅŸdÄ±rma prosesindÉ™ **Gradient Descent** kimi metodlara ehtiyac olduÄŸunu gÃ¶stÉ™rir. Bu, funksiyanÄ±n **minimum, maksimum vÉ™ ya saddle point** (saddlÉ™ÅŸmiÅŸ nÃ¶qtÉ™) tapma prosesi ilÉ™ baÄŸlÄ± ola bilÉ™r.  \n",
    "\n",
    "**QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ±** funksiyanÄ±n **differensiyalanabilÉ™n olduÄŸunu**, amma **qeyri-xÉ™tti vÉ™ ya qeyri-analitik olduÄŸunu** gÃ¶stÉ™rmÉ™z. Bu vÉ™ziyyÉ™tdÉ™ dÉ™ analitik hÉ™ll mÃ¼mkÃ¼ndÃ¼r, amma o hÉ™ll **iterativ optimallaÅŸdÄ±rma Ã¼sullarÄ± ilÉ™** tapÄ±la bilÉ™r.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ **NÉ™ticÉ™:**  \n",
    "- **QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin bÉ™rabÉ™r olmasÄ± (Clairaut-un teoremi)**: Funksiya **hamardÄ±r** vÉ™ analitik hÉ™ll mÃ¶vcuddur.  \n",
    "- **QarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ±**: Funksiya **differensiyalanabilÉ™ndir**, lakin bununla yanaÅŸÄ±, funksiyanÄ±n optimallaÅŸdÄ±rÄ±lmasÄ± Ã¼Ã§Ã¼n **iterativ metodlar** (mÉ™sÉ™lÉ™n, Gradient Descent) lazÄ±m ola bilÉ™r. Bu, funksiyanÄ±n minimum, maksimum vÉ™ ya saddle point tapmaq mÉ™qsÉ™dilÉ™ edilÉ™ bilÉ™r.  \n",
    "\n",
    "BelÉ™liklÉ™, qarÄ±ÅŸÄ±q tÃ¶rÉ™mÉ™lÉ™rin sÄ±fÄ±r olmasÄ± funksiyanÄ±n **hamar olmamasÄ±** deyil, sadÉ™cÉ™ **stasionar nÃ¶qtÉ™lÉ™rin** olduÄŸunu vÉ™ optimallaÅŸdÄ±rma Ã¼sullarÄ±nÄ±n lazÄ±m ola bilÉ™cÉ™yini gÃ¶stÉ™rir.  \n",
    "\n",
    "ÆgÉ™r baÅŸqa suallarÄ±nÄ±z varsa, mÉ™nÉ™ bildirin! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b3f49e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
